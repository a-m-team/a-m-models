# <img src="assets/am_logo.png" style="vertical-align: middle; width: 35px;"> a-m-models [![Generic badge](https://img.shields.io/badge/ğŸ¤—-am%20team-green.svg)](https://huggingface.co/a-m-team)

*Read this in [English](README_en.md).*

a-m-models æ˜¯ç”± a-m-teams å‘èµ·çš„ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œè‡´åŠ›äºå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»¥åŠé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‰æ²¿æŠ€æœ¯è¿›è¡Œæ·±å…¥æ¢ç´¢ä¸å®è·µã€‚æˆ‘ä»¬çš„å›¢é˜Ÿç”±ä¸€ç¾¤å……æ»¡çƒ­æƒ…çš„ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…ç»„æˆï¼Œèšç„¦äºå¤§æ¨¡å‹çš„ç†è®ºåˆ›æ–°ã€æ¶æ„è®¾è®¡ä»¥åŠå®æˆ˜åº”ç”¨ï¼Œæ—¨åœ¨é€æ­¥é€¼è¿‘é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å®ç°ã€‚æœ¬é¡¹ç›®æ—¨åœ¨å¼€æºåˆ†äº«æˆ‘ä»¬åœ¨å¤§æ¨¡å‹é¢†åŸŸçš„æœ€æ–°ç ”ç©¶æˆæœä¸å®è·µç»éªŒï¼Œå¸Œæœ›èƒ½å¤Ÿæ¨åŠ¨ç¤¾åŒºå¯¹AGIæŠ€æœ¯çš„æ·±åº¦äº¤æµä¸å…±åŒè¿›æ­¥ã€‚ 

## ğŸ”„ æœ€è¿‘æ›´æ–°

* [2025-05-20] å‘å¸ƒæŠ€æœ¯æŠ¥å‘Š[Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://github.com/a-m-team/a-m-models/blob/main/docs/Not%20All%20Correct%20Answers%20Are%20Equal-%20Why%20Your%20Distillation%20Source%20Matters.pdf)ï¼Œå¯¹æ¯”AM-Thinking-v1ã€Qwen3-235B-A22Bä¸DeepSeek-R1ä¸‰ä¸ªæ¨¡å‹è’¸é¦æ•ˆæœï¼ŒåŸºäºAM-Thinking-v1è’¸é¦è®­ç»ƒæ•ˆæœæœ€ä¼˜ï¼ŒåŒæ—¶åˆ†æå‘ç°å¯ä»¥æ ¹æ®é—®é¢˜éš¾åº¦è°ƒæ•´è¾“å‡ºé•¿åº¦ã€‚AM-Thinking-v1ä¸Qwen3-235B-A22Bä¸¤ä»½è’¸é¦æ•°æ®å·²å¼€æºã€‚

* [2025-05-14] å‘å¸ƒæŠ€æœ¯æŠ¥å‘Š[AM-Thinking-v1: Advancing the Frontier of
Reasoning at 32B Scale](https://arxiv.org/pdf/2505.08311)ï¼Œç»“åˆç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æ˜¾è‘—æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œåœ¨æ•°å­¦ä¸ç¼–ç¨‹ä»»åŠ¡ä¸Šè¶…è¶Š DeepSeek-R1ï¼Œé€¼è¿‘ä¸»æµ MoE æ¨¡å‹æ•ˆæœï¼Œå–å¾— Dense 32B å¼€æºæœ€ä¼˜æ°´å¹³ã€‚

* [2025-05-05] å‘å¸ƒæŠ€æœ¯æŠ¥å‘Š[Exploring the Potential of Offline RL for Reasoning in
LLMs: A Preliminary Study](https://arxiv.org/abs/2505.02142)ï¼Œæ¢ç´¢äº†Offline-RLå¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œå®éªŒç»“æœè¡¨æ˜åœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡æœ‰ä¸€è‡´æå‡ã€‚

* [2025-04-24] å‘å¸ƒæŠ€æœ¯æŠ¥å‘Š[DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)ï¼Œå¼€æºäº†çº¦4000ä¸‡æ¡ä¸åŒèƒ½åŠ›æ¨¡å‹çš„è’¸é¦æ•°æ®é›†ï¼Œæ˜¾è‘—æå‡åŸºç¡€æ¨¡å‹æ¨ç†èƒ½åŠ›ã€‚

* [2025-04-13] æ›´æ–°æŠ€æœ¯æŠ¥å‘Š[Leveraging Reasoning Model Answers to Enhance Non-Reasoning
Model Capability](https://arxiv.org/pdf/2504.09639)ï¼Œæ¢ç´¢äº†ä½¿ç”¨reasoning modelæå‡non-reasoning modelè¡¨ç°çš„æ–¹æ³•ã€‚

* [2025-04-01] æ›´æ–°æŠ€æœ¯æŠ¥å‘Š [How Difficulty-Aware Staged Reinforcement Learning Enhances LLMs' Reasoning Capabilities: A Preliminary Experimental Study](https://github.com/a-m-team/a-m-models/blob/main/docs/How-Difficulty-Aware-Staged-Reinforcement-Learning-Enhances-LLMs-Reasoning-Capabilities-A-Preliminary-Experimental-Study.pdf)ï¼Œä»‹ç»äº†ä¸€ç§åˆ†é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€æ­¥è®©æ¨¡å‹æ¥è§¦æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä»è€Œæé«˜å…¶æ¨ç†èƒ½åŠ›

* [2025-03-25] æ›´æ–°æŠ€æœ¯æŠ¥å‘Š[1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Traning](https://github.com/a-m-team/a-m-models/blob/main/docs/AM-DeepSeek-R1-Distilled-Dataset.pdf)ï¼Œå¼€æº140ä¸‡æ¡è’¸é¦æ¨ç†æ•°æ®ï¼Œå¤ç°DeepSeek-R1è’¸é¦æ¨¡å‹æ•ˆæœ

* [2025-03-25] æ›´æ–°æŠ€æœ¯æŠ¥å‘Š[Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking](https://github.com/a-m-team/a-m-models/blob/main/docs/Think-Twice.pdf)ï¼Œä»‹ç»äº†ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æµ‹è¯•é˜¶æ®µæ‰©å±•æ–¹æ³•â€”â€”å¤šè½®æ€è€ƒï¼Œå…¶æ¨åŠ¨äº†SOTAæ¨¡å‹æ•ˆæœçš„è¿›ä¸€æ­¥æå‡

## ğŸ“‘ ç ”ç©¶æŠ¥å‘Š

### [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://github.com/a-m-team/a-m-models/blob/main/docs/Not%20All%20Correct%20Answers%20Are%20Equal-%20Why%20Your%20Distillation%20Source%20Matters.pdf) [![Generic badge](https://img.shields.io/badge/ğŸ¤—-AM_thinking_v1_distilled-green.svg)](https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled) [![Generic badge](https://img.shields.io/badge/ğŸ¤—-AM_Qwen3_distilled-green.svg)](https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled)

åŸºäºAM-Thinking-v1ã€Qwen3-235B-A22Bä»¥åŠDeepSeek-R1è’¸é¦äº†ä¸‰ä»½æ¨ç†æ•°æ®ã€‚å®éªŒå‘ç°åŸºäºAM-Thinking-v1è’¸é¦æ•ˆæœæœ€ä¼˜ï¼Œå…¶ä¸­**AIME2024 84.3ï¼ŒAIME 2025 72.2, MATH500 98.4, LiveCodeBench 65.9**.

<img src="assets/Not-All-Correct-Answers-Are-Equal-Why-Your-Distillation-Source-Matters.png" alt="alt text" width="600px">

å®éªŒå‘ç°åŸºäºAM-Thinking-v1è’¸é¦è®­ç»ƒçš„æ¨¡å‹ï¼Œç›¸è¾ƒQwen3-235B-A22Bè’¸é¦è®­ç»ƒçš„æ¨¡å‹åœ¨è¾ƒç®€å•ä»»åŠ¡(å¦‚MATH500)æ¨ç†é•¿åº¦æ›´çŸ­ï¼Œåœ¨è¾ƒéš¾ä»»åŠ¡(å¦‚AIME2024 & 2025ã€LiveCodeBench)æ¨ç†è¾“å‡ºæ›´é•¿ã€‚å…¶ä¸­åŸºäºAM-Thinking-v1ä¸Qwen3-235B-A22Bè’¸é¦æ•°æ®å·²å¼€æºã€‚

#### Table: Average generation length (tokens per sample) across reasoning benchmarks

| Benchmark        | AM-Thinking-v1<sub>Distilled</sub> | Qwen3-235B-A22B<sub>Distilled</sub> | DeepSeek-R1<sub>Distilled</sub> |
|------------------|-------------------------------------|--------------------------------------|----------------------------------|
| AIME2024         | 15273.8                            | 13516.4                              | 11853.5                          |
| AIME2025         | 18199.2                            | 16975.7                              | 13495.9                          |
| MATH500          | 3495.7                             | 6429.4                               | 3613.0                           |
| LiveCodeBench    | 23426.9                            | 13576.7                              | 30731                            |



### [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/pdf/2505.08311)[![Generic badge](https://img.shields.io/badge/ğŸ¤—-AM_thinking_v1-green.svg)](https://huggingface.co/a-m-team/AM-Thinking-v1)

å½“å‰å¤§å¤šæ•°åœ¨æ¨ç†èƒ½åŠ›ä¸Šè¡¨ç°çªå‡ºçš„å¼€æºè¯­è¨€æ¨¡å‹å¤šé‡‡ç”¨Mixture-of-Expertsï¼ˆMoEï¼‰æ¶æ„ï¼Œå¦‚ Qwen3-235B-A22B å’Œ Seed1.5-Thinkingï¼Œå°½ç®¡åœ¨æ€§èƒ½ä¸Šå…·å¤‡ä¼˜åŠ¿ï¼Œä½†å…¶éƒ¨ç½²å’Œå¾®è°ƒæˆæœ¬è¾ƒé«˜ï¼Œä¸æ˜“åº”ç”¨äºèµ„æºå—é™åœºæ™¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¨ å¯†ç»“æ„çš„ä¸­ç­‰è§„æ¨¡æ¨¡å‹ï¼ˆå¦‚32Bï¼‰åœ¨æ€§èƒ½ä¸å®ç”¨æ€§ä¹‹é—´æä¾›äº†æ›´å¥½çš„å¹³è¡¡ï¼Œä½†ç›¸å…³å·¥ä½œä»ç›¸å¯¹è¾ƒå°‘ã€‚

åŸºäºè¿™ä¸€åŠ¨æœºï¼Œæˆ‘ä»¬æ„å»ºäº† **AM-Thinking-v1**. è¯¥æ¨¡å‹ä½¿ç”¨å…¬å¼€æ•°æ®ï¼Œé€šè¿‡æœ‰ç›‘ç£å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆçš„åè®­ç»ƒæµç¨‹ä¼˜åŒ–æ¨ç†ä¸ä»£ç èƒ½åŠ›ã€‚

<img src="assets/am-thinking-v1-benchmark.png" alt="alt text" width="600px">

å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAM-Thinking-v1 åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼š**AIME 2024 å¾—åˆ† 85.3ï¼ŒAIME 2025 å¾—åˆ† 74.4ï¼ŒLiveCodeBench å¾—åˆ† 70.3**ï¼Œè¶…è¿‡ DeepSeek-R1ï¼Œå¹¶æ¥è¿‘ MoE æ¶æ„çš„æœ€å¼ºæ¨¡å‹ï¼Œæ˜¯å½“å‰Dense 32Bæœ€ä¼˜æ¨¡å‹ã€‚ç»“æœè¡¨æ˜ï¼Œå¾—ç›Šäºç²¾ç»†çš„è®­ç»ƒæµç¨‹ï¼Œ32B è§„æ¨¡çš„å¼€æºç¨ å¯†æ¨¡å‹äº¦å¯åœ¨é«˜éš¾åº¦æ¨ç†ä»»åŠ¡ä¸­å®ç°ç«äº‰æ€§èƒ½ã€‚

<img src="assets/am-thinking-v1-results_with_params.jpg" alt="alt text" width="600px">


### [Exploring the Potential of Offline RL for Reasoning inLLMs: A Preliminary Study](https://github.com/a-m-team/a-m-models/blob/main/docs/Exploring-the-Potential-of-Offline-RL-for-Reasoning-in-LLMs-A-Preliminary-Study.pdf)

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°æŒç»­æå‡ï¼Œå½“å‰çš„ä¸»æµæ–¹æ³•ä¸»è¦ä¾èµ–åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOnline RLï¼‰ï¼Œç„¶è€Œè¿™äº›æ–¹æ³•é€šå¸¸ä¼´éšè¾ƒé«˜çš„è®¡ç®—æˆæœ¬å’Œå¤æ‚æ€§ã€‚ç›¸è¾ƒè€Œè¨€ï¼Œç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOffline RLï¼‰æ–¹æ³•å› å…¶ç®€æ´é«˜æ•ˆè€Œå±•ç°å‡ºæ½œåœ¨çš„ä¼˜åŠ¿ï¼Œä½†åœ¨é•¿ä¸Šä¸‹æ–‡æ¨ç†é¢†åŸŸå´æœªè·å¾—å……åˆ†æ¢ç´¢ã€‚

é’ˆå¯¹è¿™ä¸€ç ”ç©¶ç©ºç™½ï¼Œæœ¬è®ºæ–‡æ¢è®¨äº†Offline RLæ–¹æ³•ï¼Œå°¤å…¶æ˜¯ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆDirect Preference Optimization, DPOï¼‰åŠå…¶å¯¹è¾“å‡ºé•¿åº¦ä¸æ•æ„Ÿçš„å˜ä½“LD-DPOï¼Œåœ¨æå‡LLMsæ¨ç†èƒ½åŠ›ä¸Šçš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œåœ¨å¤šä¸ªæ¨ç†åŸºå‡†ä¸ŠéªŒè¯äº†è¿™äº›æ›´ä¸ºç®€æ´çš„Offline RLæ–¹æ³•èƒ½å¤Ÿæ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼Œå¹³å‡æå‡è¾¾åˆ°**3.3%**ï¼Œå…¶ä¸­arena-hardåŸºå‡†æµ‹è¯•ä¸­æå‡è¾¾åˆ°**10.1%**ã€‚

æ­¤å¤–ï¼Œæœ¬ç ”ç©¶åˆ†æäº†DPOæ–¹æ³•å¯¹äºè¾“å‡ºé•¿åº¦çš„æ•æ„Ÿæ€§ï¼Œå¼ºè°ƒåœ¨å»¶é•¿æ¨ç†æ–‡æœ¬é•¿åº¦æ—¶éœ€è¦å…³æ³¨å†…å®¹çš„è¯­ä¹‰ä¸°å¯Œæ€§ï¼Œè€Œéç›²ç›®å¢åŠ é•¿åº¦ï¼Œå¦åˆ™å¯èƒ½ä¼šå¯¹æ¨¡å‹æ€§èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚

<img src="assets/Exploring-the-Potential-of-Offline-RL-for-Reasoning-in-LLMs-A-Preliminary-Study.png" alt="alt text" width="600px">


### [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)[![Generic badge](https://img.shields.io/badge/ğŸ¤—-AM_DeepSeek_Distilled_40M-green.svg)](https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M)

å°½ç®¡è¿‘æœŸå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œä½†å¯¹åŸºç¡€æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹å’Œæ•°æ®è´¨é‡çš„æ·±å…¥ç†è§£ä»ç„¶ä¸è¶³ã€‚ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªåŒ…å«çº¦**334ä¸‡**ä¸ªä¸é‡å¤é—®é¢˜å’Œ**4000ä¸‡**æ¡ç”±ä¸åŒèƒ½åŠ›æ¨¡å‹å¤šæ¬¡è’¸é¦ç­”æ¡ˆçš„å¤§è§„æ¨¡æ¨ç†æ•°æ®é›†ã€‚é€šè¿‡å¼•å…¥é€šè¿‡ç‡ï¼ˆPass Rateï¼‰å’Œå˜å¼‚ç³»æ•°ï¼ˆCoefficient of Variationï¼‰ï¼Œæˆ‘ä»¬ç²¾å‡†é€‰æ‹©å…·æœ‰æœ€é«˜å­¦ä¹ æ½œåŠ›çš„è®­ç»ƒæ•°æ®ï¼Œä»¥æå‡æ¨ç†èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†å·²å…¬å¼€åœ¨ <https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M>ã€‚

åœ¨AIME2024ä¸Šï¼Œæˆ‘ä»¬çš„72Bæ¨¡å‹**ä»…é€šè¿‡SFT**è¾¾åˆ°äº†79.2åˆ†ï¼›32Bæ¨¡å‹è¾¾åˆ°75.8åˆ†ï¼Œè¿›ä¸€æ­¥é€€ç«è®­ç»ƒè¾¾åˆ°77.9åˆ†ï¼Œæ¥è¿‘å¼€æºæœ€ä¼˜æ°´å¹³ã€‚

<img src="assets/DeepDistill.png" alt="alt text" width="600px">


### [Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability](https://arxiv.org/pdf/2504.09639)

è¿‘æœŸå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„è¿›å±•ï¼Œä¾‹å¦‚ DeepSeek-R1 å’Œ OpenAI-o1ï¼Œå·²å±•ç¤ºäº†test time scalingçš„æ˜¾è‘—æœ‰æ•ˆæ€§ï¼Œåœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†å®è´¨æ€§çš„æ€§èƒ½æå‡ã€‚è¿™äº›å…ˆè¿›æ¨¡å‹åˆ©ç”¨å®¡æ…çš„"æ€è€ƒ"æ­¥éª¤ç³»ç»Ÿåœ°æé«˜ç­”æ¡ˆè´¨é‡ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºåˆ©ç”¨è¿™äº›ç”±reasoning modelç”Ÿæˆçš„é«˜è´¨é‡è¾“å‡ºï¼Œæ¥æ”¹è¿›è®¡ç®—éœ€æ±‚è¾ƒä½ã€éæ¨ç†çš„æ¨¡å‹ã€‚æˆ‘ä»¬æ¢ç´¢å¹¶æ¯”è¾ƒäº†åˆ©ç”¨æ¨ç†æ¨¡å‹äº§ç”Ÿçš„ç­”æ¡ˆæ¥è®­ç»ƒå’Œæ”¹è¿›éæ¨ç†æ¨¡å‹çš„æ–¹æ³•ã€‚é€šè¿‡åœ¨æ—¢å®šåŸºå‡†ä¸Šè¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å®éªŒï¼Œæˆ‘ä»¬åœ¨å„ç§åŸºå‡†ä¸Šå–å¾—äº†æŒç»­çš„æ”¹è¿›ï¼Œå¼ºè°ƒäº†è¿™ç§æ–¹æ³•åœ¨æå‡non-reasoning modelç›´æ¥å›ç­”é—®é¢˜çš„èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚

1. **æ–¹æ³•**: å¯¹æ¯”äº†ä¸‰ç§åˆ©ç”¨reasoning-modelå†…å®¹çš„æ–¹æ³•:
   - **æ–¹æ³•1**: ä½¿ç”¨åŸç”Ÿçš„non-reasoning modeläº§ç”Ÿçš„å›ç­”ï¼›
   - **æ–¹æ³•2**: ä½¿ç”¨reasoning modelçš„'answer'éƒ¨åˆ†ï¼›
   - **æ–¹æ³•3**: think summarization: æ€»ç»“reasoning model çš„think éƒ¨åˆ† å’Œ answeréƒ¨åˆ†æ‹¼åœ¨ä¸€èµ·ã€‚

2. **ç»“è®º**: æ­£ç¡®ä½¿ç”¨reasoning modelçš„å›å¤å†…å®¹å¯ä»¥å¢å¼ºnon-reasoning modelçš„èƒ½åŠ›ï¼Œå…·ä½“æ•ˆæœå¦‚å›¾æ‰€ç¤ºã€‚ç„¶è€Œï¼Œè‹¥æ–¹æ³•ä¸å½“ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹çš„æŸäº›æŒ‡æ ‡ä¸‹é™ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­éœ€è¦æ ¹æ®ä¸åŒåœºæ™¯é‡‡ç”¨ç‰¹å®šç­–ç•¥æ¥æå‡non-reasoning modelçš„èƒ½åŠ›ã€‚

<img src="assets/Leveraging-Reasoning-Model-Answers-to-Enhance-Non-Reasoning-Model-Capability.png" alt="alt text" width="600px">


### [How Difficulty-Aware Staged Reinforcement Learning Enhances LLMs' Reasoning Capabilities: A Preliminary Experimental Study](https://github.com/a-m-team/a-m-models/blob/main/docs/How-Difficulty-Aware-Staged-Reinforcement-Learning-Enhances-LLMs-Reasoning-Capabilities-A-Preliminary-Experimental-Study.pdf)[![Generic badge](https://img.shields.io/badge/ğŸ¤—-AM_Math_Difficulty_RL-green.svg)](https://huggingface.co/datasets/a-m-team/AM-Math-Difficulty-RL)

æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¨ç†èƒ½åŠ›çš„æ•ˆç‡å’Œè§„æ¨¡æ˜¯äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚æœ¬æ–‡ç ”ç©¶äº†éš¾åº¦æ„ŸçŸ¥åˆ†é˜¶æ®µå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç­–ç•¥å¦‚ä½•æå‡LLMæ€§èƒ½ã€‚æˆ‘ä»¬è¡¨æ˜ï¼ŒåŸºäºéš¾åº¦ç­‰çº§é€‰æ‹©è®­ç»ƒæ•°æ®æœ‰åŠ©äºå¼ºåŒ–å­¦ä¹ ä¼˜åŒ–ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åˆ†é˜¶æ®µè®­ç»ƒæ–¹æ³•ï¼Œé€æ­¥è®©æ¨¡å‹æ¥è§¦æ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œä»è€Œæé«˜å…¶æ¨ç†èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç»“æœè¿˜å¼ºè°ƒäº†åœ¨æ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šè®­ç»ƒæ¨¡å‹çš„æ˜¾è‘—å¥½å¤„ã€‚

#### 1. æ•°æ®éš¾åº¦é€‰æ‹©

æ ¹æ®é€‚å½“çš„éš¾åº¦æŒ‡æ ‡ç²¾å¿ƒé€‰æ‹©RLè®­ç»ƒæ•°æ®è‡³å…³é‡è¦ã€‚é€‚ä¸­çš„éš¾åº¦æ°´å¹³èƒ½å¤Ÿæé«˜å­¦ä¹ æ•ˆç‡ï¼Œå¹³è¡¡å……åˆ†æŒ‘æˆ˜ä¸é¿å…ç”¨è¿‡äºå›°éš¾çš„æƒ…å¢ƒå‹å€’å­¦ä¹ è¿‡ç¨‹ä¹‹é—´çš„éœ€æ±‚ã€‚

<img src="assets/staged-RL-data-difficulty.png" alt="alt text" width="600px">

#### 2. åˆ†é˜¶æ®µè®­ç»ƒ

é€šè¿‡é€‰æ‹©é€‚å½“å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°æ®å¹¶ç»“åˆåˆ†é˜¶æ®µè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥æ˜¾è‘—æé«˜LLMåœ¨æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ï¼ˆç”±äºæ²¡åŠ å…¥ä¸ä»£ç ç›¸å…³çš„è®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹åœ¨LiveCodeBenchä¸Šçš„è¡¨ç°ä¸åŸºç¡€æ¨¡å‹åŸºæœ¬ç›¸åŒã€‚ï¼‰

<img src="assets/staged-RL-2stage.png" alt="alt text" width="600px">

#### 3. æ•°å­¦å’Œä»£ç çš„åŒæ—¶è®­ç»ƒ

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ··åˆæ•°å­¦æ¨ç†å’Œä»£ç ç”Ÿæˆä»»åŠ¡å¯ä»¥å¸¦æ¥è·¨é¢†åŸŸçš„æå‡ï¼Œå¼ºæœ‰åŠ›åœ°è¯æ˜äº†å¤šé¢†åŸŸè®­ç»ƒçš„å¥½å¤„ã€‚

<img src="assets/staged-RL-math-code.png" alt="alt text" width="600px">

### [Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking](https://github.com/a-m-team/a-m-models/blob/main/docs/Think-Twice.pdf)

è¿‘å¹´æ¥ï¼Œä»¥OpenAI-o1å’ŒDeepSeek-R1ä¸ºä»£è¡¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œè¿™äº›è¿›å±•è¡¨æ˜ï¼Œé€šè¿‡æµ‹è¯•é˜¶æ®µæ‰©å±•æ¨ç†æµç¨‹ï¼ˆtest-time scalingï¼‰ï¼Œå¯æ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ã€‚ç„¶è€Œï¼Œç›®å‰çš„æ¨¡å‹ä»å—åˆ°å¤„ç†é•¿æ–‡æœ¬èƒ½åŠ›å’Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒæ•ˆç‡çš„é™åˆ¶ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ä¸”æœ‰æ•ˆçš„æµ‹è¯•é˜¶æ®µæ‰©å±•æ–¹æ³•â€”â€”å¤šè½®æ€è€ƒï¼ˆMulti-round Thinkingï¼‰ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†æ¨¡å‹å…ˆå‰çš„å›ç­”ä½œä¸ºä¸‹ä¸€è½®æ¨ç†çš„æç¤ºï¼ˆpromptsï¼‰ï¼Œè¿­ä»£åœ°ç²¾è¿›æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨åŒ…æ‹¬QwQ-32Bå’ŒDeepSeek-R1åœ¨å†…çš„å¤šä¸ªæ¨¡å‹ä¸Šçš„å¤§é‡å®éªŒè¡¨æ˜ï¼Œå¤šè½®æ€è€ƒèƒ½å¤Ÿåœ¨AIME 2024ã€MATH-500ã€GPQA-diamondå’ŒLiveCodeBenchç­‰å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­ç¨³å®šæå‡æ¨¡å‹è¡¨ç°ã€‚ä¾‹å¦‚ï¼Œåœ¨AIME 2024æ•°æ®é›†ä¸­ï¼ŒQwQ-32Bçš„å‡†ç¡®ç‡ä»ç¬¬ä¸€è½®çš„80.3%æé«˜åˆ°ç¬¬äºŒè½®çš„82.1%ï¼ŒDeepSeek-R1ä¹Ÿè¡¨ç°å‡ºäº†ç±»ä¼¼çš„æå‡ï¼Œä»79.7%æé«˜åˆ°82.0%ã€‚è¿™äº›ç»“æœè¯æ˜ï¼Œå¤šè½®æ€è€ƒæ˜¯ä¸€ç§é€‚ç”¨å¹¿æ³›ã€å®æ–½ç®€å•ä¸”æœ‰æ•ˆæå‡æ¨¡å‹è¡¨ç°çš„æ–¹æ³•ï¼Œå½°æ˜¾å‡ºè¯¥æ–¹æ³•åœ¨æœªæ¥æµ‹è¯•é˜¶æ®µæ‰©å±•æŠ€æœ¯å‘å±•ä¸­çš„å·¨å¤§æ½œåŠ›ã€‚

The key prompt:
```
Original question prompt
The assistantâ€™s previous answer is: <answer> last round answer </answer>, and please re-answer.
```

<img src="assets/Think-Twice-QwQ.png" alt="alt text" width="600px">
<img src="assets/Think-Twice-DeepSeek-R1.png" alt="alt text" width="600px">


### ä¸åŒåŸºå‡†æµ‹è¯•ä¸­å•è½®æ€è€ƒï¼ˆç¬¬1è½®ï¼‰ä¸å¤šè½®æ€è€ƒï¼ˆç¬¬2-4è½®ï¼‰çš„æ¨¡å‹è¡¨ç°å¯¹æ¯”ï¼ˆpass@1ï¼‰

| **Model**                              | **Round** | **AIME 2024 pass@1** | **MATH500 pass@1** | **GPQA-Diamond pass@1** | **LiveCodeBench pass@1** | **Average** |
|----------------------------------------|-----------|----------------------|--------------------|-------------------------|--------------------------|-------------|
| **Deepseek-R1**                        | 1         | 79.7                 | 97.6               | 74.0                    | 65.3                     | 79.2        |
|                                        | **2**     | **82.0**             | **97.6**           | **74.8**                | **67.1**                 | **80.4**    |
| **QwQ-32B**                            | 1         | 80.3                 | 97.2               | 65.9                    | 63.0                     | 76.6        |
|                                        | 2         | 82.1                 | 97.8               | 67.2                    | 64.7                     | 78.0        |
|                                        | 3         | 82.8                 | 97.8               | 67.5                    | 65.2                     | 78.3        |
|                                        | **4**     | **83.1**             | **97.7**           | **68.1**                | **66.0**                 | **78.7**    |
| **DeepSeek-R1-Distill-Qwen-32B**       | 1         | 72.0                 | 96.0               | 60.1                    | 57.0                     | 71.3        |
|                                        | **2**     | **75.1**             | **96.3**           | **61.3**                | **57.6**                 | **72.6**    |
| **DeepSeek-R1-Distill-Qwen-7B**        | 1         | 56.9                 | 93.4               | 49.2                    | 35.0                     | 58.6        |
|                                        | **2**     | **58.4**             | **93.9**           | **49.4**                | **36.7**                 | **59.6**    |
| **AM-Distill-Qwen-32B**                | 1         | 72.8                 | 96.2               | 62.3                    | 58.3                     | 72.4        |
|                                        | **2**     | **76.7**             | **97.2**           | **62.8**                | **60.2**                 | **74.2**    |

---
---

### [1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Traning](https://github.com/a-m-team/a-m-models/blob/main/docs/AM-DeepSeek-R1-Distilled-Dataset.pdf) [![Generic badge](https://img.shields.io/badge/ğŸ¤—-1.4M-green.svg)](https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4M)

AM-DeepSeek-R1-Distilled æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡ã€å¸¦æœ‰æ¨ç†è¿‡ç¨‹çš„é€šç”¨æ¨ç†ä»»åŠ¡æ•°æ®é›†ï¼ŒåŒ…å«å¤§é‡é«˜è´¨é‡ä¸”å…·å¤‡æŒ‘æˆ˜æ€§çš„æ¨ç†é—®é¢˜ã€‚è¿™äº›é—®é¢˜æ”¶é›†è‡ªå¤šä¸ªå¼€æºæ•°æ®é›†ï¼Œç»è¿‡è¯­ä¹‰å»é‡å’Œç²¾ç»†æ¸…ç†ï¼Œä»¥æ¶ˆé™¤å¯èƒ½çš„æµ‹è¯•é›†æ±¡æŸ“é£é™©ã€‚æ•°æ®é›†ä¸­æ‰€æœ‰çš„ç­”æ¡ˆå‡ç”±æ¨ç†æ¨¡å‹ï¼ˆä¸»è¦ä¸º DeepSeek-R1ï¼‰è’¸é¦è€Œæˆï¼Œå¹¶ç»è¿‡ä¸¥æ ¼çš„éªŒè¯æµç¨‹ï¼šæ•°å­¦é—®é¢˜é€šè¿‡ä¸æ ‡å‡†ç­”æ¡ˆå¯¹æ¯”è¿›è¡ŒéªŒè¯ï¼Œä»£ç é—®é¢˜é€šè¿‡æµ‹è¯•ç”¨ä¾‹è¿›è¡Œæ ¸éªŒï¼Œè€Œå…¶ä»–ç±»å‹ä»»åŠ¡åˆ™é€šè¿‡å¥–åŠ±æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚åŸºäºè¯¥æ•°æ®é›†ä»…ä½¿ç”¨ç®€å•ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒçš„ AM-Distill-Qwen-32B æ¨¡å‹ï¼Œåœ¨ AIME2024ã€MATH-500ã€GPQA-Diamond ä»¥åŠ LiveCodeBench å››é¡¹åŸºå‡†æµ‹è¯•ä¸Šï¼Œå‡è¶…è¶Šäº†DeepSeek-R1-Distill-Qwen-32B æ¨¡å‹ã€‚ä¸ºäº†æ¨åŠ¨æ›´å¼ºå¤§çš„æ¨ç†å¯¼å‘å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‘å±•ï¼Œæˆ‘ä»¬å¼€æºäº†è¿™140ä¸‡æ¡é—®é¢˜åŠå…¶å¯¹åº”çš„ç­”æ¡ˆã€‚è¯¥æ•°æ®é›†å·²å…¬å¼€åœ¨ <https://huggingface.co/datasets/a-m-team/AM-DeepSeek-R1-Distilled-1.4Mã€‚>

<img src="assets/AM-DeepSeek-R1-Distilled.jpeg" alt="alt text" width="600px">

## Citation

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©ï¼Œæ¬¢è¿ç»™æˆ‘ä»¬ç‚¹ä¸ªæ˜Ÿ :star:, å¹¶å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œ:pencil:

```BibTeX
@misc{ji2025amthinkingv1advancingfrontierreasoning,
      title={AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale}, 
      author={Yunjie Ji and Xiaoyu Tian and Sitong Zhao and Haotian Wang and Shuaiting Chen and Yiping Peng and Han Zhao and Xiangang Li},
      year={2025},
      eprint={2505.08311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.08311}, 
}

@misc{tian2025exploringpotentialofflinerl,
      title={Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study}, 
      author={Xiaoyu Tian and Sitong Zhao and Haotian Wang and Shuaiting Chen and Yiping Peng and Yunjie Ji and Han Zhao and Xiangang Li},
      year={2025},
      eprint={2505.02142},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.02142}, 
}

@misc{tian2025deepdistillenhancingllmreasoning,
      title={DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training}, 
      author={Xiaoyu Tian and Sitong Zhao and Haotian Wang and Shuaiting Chen and Yiping Peng and Yunjie Ji and Han Zhao and Xiangang Li},
      year={2025},
      eprint={2504.17565},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.17565}, 
}

@misc{wang2025leveragingreasoningmodelanswers,
      title={Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability}, 
      author={Haotian Wang and Han Zhao and Shuaiting Chen and Xiaoyu Tian and Sitong Zhao and Yunjie Ji and Yiping Peng and Xiangang Li},
      year={2025},
      eprint={2504.09639},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.09639}, 
}

@misc{ji2025difficultyawarestagedreinforcementlearning,
      title={How Difficulty-Aware Staged Reinforcement Learning Enhances LLMs' Reasoning Capabilities: A Preliminary Experimental Study}, 
      author={Yunjie Ji and Sitong Zhao and Xiaoyu Tian and Haotian Wang and Shuaiting Chen and Yiping Peng and Han Zhao and Xiangang Li},
      year={2025},
      eprint={2504.00829},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.00829}, 
}

@misc{tian2025thinktwiceenhancingllm,
      title={Think Twice: Enhancing LLM Reasoning by Scaling Multi-round Test-time Thinking}, 
      author={Xiaoyu Tian and Sitong Zhao and Haotian Wang and Shuaiting Chen and Yunjie Ji and Yiping Peng and Han Zhao and Xiangang Li},
      year={2025},
      eprint={2503.19855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.19855}, 
}

@misc{zhao202514millionopensourcedistilled,
      title={1.4 Million Open-Source Distilled Reasoning Dataset to Empower Large Language Model Training}, 
      author={Han Zhao and Haotian Wang and Yiping Peng and Sitong Zhao and Xiaoyu Tian and Shuaiting Chen and Yunjie Ji and Xiangang Li},
      year={2025},
      eprint={2503.19633},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.19633}, 
}


```
